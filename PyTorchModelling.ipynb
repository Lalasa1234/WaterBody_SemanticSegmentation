{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lalasa1234/WaterBody_SemanticSegmentation/blob/main/PyTorchModelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "klDBwlurzifb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.io import read_image\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "SIZE = 128\n",
        "SEED = 42\n",
        "BATCH = 32\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ow6K1QO5zlPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da896d85-1410-4cdd-8954-c5a199c40fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount =True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLReBFtyzogy"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/WaterBody_ImageSegmentation')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dz1dqGAz2a4"
      },
      "outputs": [],
      "source": [
        "real_data = sorted(glob.glob('/content/drive/MyDrive/Colab Notebooks/WaterBody_ImageSegmentation/Water Bodies Dataset/Images/' + '*.jpg'))\n",
        "masked_data = sorted(glob.glob('/content/drive/MyDrive/Colab Notebooks/WaterBody_ImageSegmentation/Water Bodies Dataset/Masks/' + '*.jpg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDf4F0wCAk_H"
      },
      "outputs": [],
      "source": [
        "print ('Real Data - ', Image.open(real_data[2]).mode)\n",
        "print (asarray(Image.open(real_data[2])).shape, np.transpose(Image.open(real_data[2]),(2,0,1)).shape)\n",
        "# Masked data looks grayscaled. hence this needs to be changed\n",
        "test_mask = Image.open(masked_data[2])\n",
        "print ('Masked Data - ', test_mask.getbands(), asarray(test_mask.convert('L')).shape, np.expand_dims(asarray(test_mask.convert('L')), 0).shape)\n",
        "print ('Min and max of real data is ',asarray(Image.open(real_data[2])).min(),asarray(Image.open(real_data[2])).max())\n",
        "print ('Min and max is mask data is ',asarray(test_mask).min(),asarray(test_mask).max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCv7b0-2BbmN"
      },
      "source": [
        "### Display real and masked of some images side by side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl3pEyriBgad"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.subplot(341 + i)\n",
        "  plt.imshow(Image.open(real_data[i]))\n",
        "  plt.subplot(342 + i)\n",
        "  plt.imshow(Image.open(masked_data[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7x1kvna7VFc"
      },
      "source": [
        "### Check the resize function by observing the axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_wpjvTZuiJW"
      },
      "outputs": [],
      "source": [
        "resizer = v2.Resize((512, 512))\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(Image.open(real_data[0]))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(resizer(Image.open(real_data[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzrG9aba96Tu"
      },
      "source": [
        "### Testing the Random Horizontal Flip on the same image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nru0cNrdu_M-"
      },
      "outputs": [],
      "source": [
        "# Flip takes the probaility as param., so for every run, image may or may not get flipped\n",
        "hor_flipper = v2.RandomHorizontalFlip(p=0.5)\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(Image.open(real_data[0]))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(hor_flipper(Image.open(real_data[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYpPdmhuwEx1"
      },
      "source": [
        "### Test Random Rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d26EOU1gwL-v"
      },
      "outputs": [],
      "source": [
        "rot = v2.RandomRotation((0,30))\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(Image.open(real_data[0]))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(rot(Image.open(real_data[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VwcgSqtxLZS"
      },
      "source": [
        "### Test Vertical Flip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypEghzO8xOIX"
      },
      "outputs": [],
      "source": [
        "ver_flipper = v2.RandomVerticalFlip(p=0.1)\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(Image.open(real_data[0]))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(ver_flipper(Image.open(real_data[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs44cedW-j2T"
      },
      "source": [
        "### Check color changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7m0Fju33684"
      },
      "outputs": [],
      "source": [
        "col_changer = v2.ColorJitter(brightness=1,contrast = 1, saturation = 0.3, hue = 0.5)\n",
        "# contrast determines the no. of shades. saturation det. the intensity, hue should be less than 0.5\n",
        "# hue shifts the color values\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(Image.open(real_data[0]))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(col_changer(Image.open(real_data[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNcYFW5FVO68"
      },
      "source": [
        "### Check greyscale transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7at9zKvYU28z"
      },
      "outputs": [],
      "source": [
        "gray = v2.Grayscale(3)\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(Image.open(real_data[2]))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(gray(Image.open(real_data[2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_1AOh0jXGwt"
      },
      "outputs": [],
      "source": [
        "print (asarray(Image.open(real_data[2])).shape)\n",
        "print (asarray(gray(Image.open(real_data[2]))).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6EWNcKXp0m5"
      },
      "source": [
        "### **Building the Model: U-Net on PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVP-uA9t1RIO"
      },
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IycMoyL_VdY"
      },
      "outputs": [],
      "source": [
        "class SatelliteData(Dataset):\n",
        "  def __init__(self, real_data, masked_data, transform = None, mask_transform = None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.real_data = real_data\n",
        "    self.masked_data = masked_data\n",
        "\n",
        "    self.transform = transform\n",
        "    self.mask_transform = mask_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.real_data)\n",
        "\n",
        "  # We locate the file, convert to tensor using read_image, call the transforms (if appl.) and return\n",
        "  def __getitem__(self, index):\n",
        "    image = read_image(self.real_data[index])\n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "      image = image/250.0\n",
        "\n",
        "    mask = read_image(self.masked_data[index])\n",
        "    if self.mask_transform is not None:\n",
        "      mask = self.transform(mask)\n",
        "      mask = self.mask_transform(mask)\n",
        "      mask = mask/255.0\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8FaPVuzd057"
      },
      "outputs": [],
      "source": [
        "transform = v2.Compose([v2.Resize((SIZE,SIZE)),v2.RandomHorizontalFlip(),v2.RandomRotation((0,30)),\n",
        "                        v2.RandomVerticalFlip(0.1)])\n",
        "\n",
        "test_transform = v2.Compose([v2.Resize((SIZE,SIZE))])\n",
        "\n",
        "mask_transform = v2.Grayscale(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W--AUmwgszpX"
      },
      "source": [
        "Testing whether the dataset works or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5G8fWFfrVzd"
      },
      "outputs": [],
      "source": [
        "obj = SatelliteData(real_data, masked_data)\n",
        "print (obj.masked_data[0:2], obj.masked_data[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkgnYB9WXyNT"
      },
      "outputs": [],
      "source": [
        "obj.transform, obj.mask_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wek3X9vNX0Fi"
      },
      "outputs": [],
      "source": [
        "# test the len func\n",
        "len(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fpjis57sZXMF"
      },
      "outputs": [],
      "source": [
        "# test the getitem func\n",
        "print (obj[0][0].shape, obj[0][1].shape)\n",
        "obj[0][0].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv4cg1jme8z9"
      },
      "outputs": [],
      "source": [
        "# test the transformation\n",
        "obj = SatelliteData(real_data, masked_data, transform, mask_transform)\n",
        "print (obj.masked_data[0:2], obj.masked_data[0:2])\n",
        "print (obj.transform, obj.mask_transform)\n",
        "\n",
        "obj[0][0].shape, obj[0][1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jerBE_1Bf4jA"
      },
      "outputs": [],
      "source": [
        "# matplotlib doesn't understand normalization of RGB images, so either convert them to int, or normalize again\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(obj[12][0].permute(1,2,0))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(obj[12][1].permute(1,2,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAQWQcQ6tPV1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(real_data, masked_data, test_size=0.1, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDR2CYABusnT"
      },
      "outputs": [],
      "source": [
        "train_data = SatelliteData(X_train, y_train, transform, mask_transform)\n",
        "test_data = SatelliteData(X_test, y_test,test_transform,mask_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8ap1zpMwWWk"
      },
      "outputs": [],
      "source": [
        "# num_workers represents the no. of batches that will be parallelly loaded: Windows needs to have a main() imple.- Not exce. now\n",
        "train_loader = DataLoader(train_data,batch_size=BATCH,shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=BATCH, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO3nKjlcdMJv"
      },
      "outputs": [],
      "source": [
        "print (len(train_data), len(train_loader))\n",
        "print (len(test_data), len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUK0M2WPdREY"
      },
      "outputs": [],
      "source": [
        "# test the train data loader\n",
        "for i, (x,y) in enumerate(train_loader):\n",
        "  print (x.shape, x[0].shape)\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(x[1].permute(1,2,0))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(y[1].permute(1,2,0))\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the val data loader\n",
        "for i, (x,y) in enumerate(test_loader):\n",
        "  print (x.shape, x[0].shape)\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(x[1].permute(1,2,0))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(y[1].permute(1,2,0))\n",
        "  break"
      ],
      "metadata": {
        "id": "dErzI3TTp3IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S_HE4Grj_0b"
      },
      "source": [
        "### Defining the model for binary segmentation\n",
        "\n",
        "Understanding UNet: Every pixel is broken into water or non-water class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjFs_r-PhC9d"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "\n",
        "  def __init__(self,num_in_chan, num_out_chan):\n",
        "    super().__init__()\n",
        "\n",
        "    # same padding ensures that the size of o/p feature map is the same as i/p\n",
        "    # batch_norm has its own bias paramater called the shift_param (alpha and beta)\n",
        "    # The shift_params overwrite the Conv2D's bias, hence we remove the redundant bias\n",
        "    self.conv_block = nn.Sequential(\n",
        "    nn.Conv2d(num_in_chan,num_out_chan,padding='same',kernel_size=(3,3),bias= False),\n",
        "    nn.BatchNorm2d(num_out_chan),\n",
        "    nn.ReLU(inplace = True),\n",
        "\n",
        "    nn.Conv2d(num_out_chan,num_out_chan,padding='same',kernel_size=(3,3),bias=False),\n",
        "    nn.BatchNorm2d(num_out_chan),\n",
        "    nn.ReLU(inplace = True))\n",
        "\n",
        "  def forward(self,x):\n",
        "      return self.conv_block(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpG9OE3-E0Vk"
      },
      "outputs": [],
      "source": [
        "# Create the Downsampling Block\n",
        "class encoder(nn.Module):\n",
        "  def __init__(self,num_in_chan,num_out_chan):\n",
        "    super().__init__()\n",
        "    self.conv_block = DoubleConv(num_in_chan,num_out_chan)\n",
        "    self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    side = self.conv_block(x)\n",
        "    down = self.pool(side)\n",
        "    return side,down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j53ZCkMKLBT"
      },
      "outputs": [],
      "source": [
        "# Create the Upsampling Block\n",
        "class decoder(nn.Module):\n",
        "  def __init__(self,num_in_chan,num_out_chan):\n",
        "    super().__init__()\n",
        "    # stride = 2 in order to maintain the channel size to its corresponding conv2d layer\n",
        "    # for ConvTranspose2d, padding should be an int/tuple and not string\n",
        "    self.conv1 = nn.ConvTranspose2d(num_in_chan,num_out_chan,kernel_size=2,stride=2,padding=0)\n",
        "    # no. of channels is twice to align with the channel dim. input = down + its corresponding conv2d side\n",
        "    self.conv2 = DoubleConv(num_out_chan*2,num_out_chan)\n",
        "\n",
        "  def forward(self,x,side):\n",
        "    x = self.conv1(x)\n",
        "    # The channels from side and down are concatenated. Shape is (Batch, Channel, Height, Width) hence dim=1\n",
        "    x = torch.cat([x,side],dim=1)\n",
        "    up = self.conv2(x)\n",
        "    return up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDT428FcDxaP"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder1 = encoder(3,64)\n",
        "    self.encoder2 = encoder(64,128)\n",
        "    self.encoder3 = encoder(128,256)\n",
        "    self.encoder4 = encoder(256,512)\n",
        "\n",
        "    self.b1 = DoubleConv(512,1024)\n",
        "\n",
        "    self.decoder1 = decoder(1024,512)\n",
        "    self.decoder2 = decoder(512,256)\n",
        "    self.decoder3 = decoder(256,128)\n",
        "    self.decoder4 = decoder(128,64)\n",
        "\n",
        "    # channels = 1 as this is a binary segmentation- water as the foreground vs. rem. as the background\n",
        "    # for n classes, use channels = n\n",
        "    self.output = nn.Conv2d(64,1,kernel_size=(1,1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.encoder1.side1,self.encoder1.down1 = self.encoder1(x)\n",
        "    self.encoder2.side2,self.encoder2.down2 = self.encoder2(self.encoder1.down1)\n",
        "    self.encoder3.side3,self.encoder3.down3 = self.encoder3(self.encoder2.down2)\n",
        "    self.encoder4.side4,self.encoder4.down4 = self.encoder4(self.encoder3.down3)\n",
        "\n",
        "    # we don't call sigmoid here as we use the loss function BCEWithLogitsLoss()\n",
        "    self.b1_out = self.b1(self.encoder4.down4)\n",
        "\n",
        "    self.decoder1_out = self.decoder1(self.b1_out,self.encoder4.side4)\n",
        "    self.decoder2_out = self.decoder2(self.decoder1_out,self.encoder3.side3)\n",
        "    self.decoder3_out = self.decoder3(self.decoder2_out,self.encoder2.side2)\n",
        "    self.decoder4_out = self.decoder4(self.decoder3_out,self.encoder1.side1)\n",
        "\n",
        "    self.output_out = self.output(self.decoder4_out)\n",
        "    return self.output_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eecBA-yEWVvN"
      },
      "source": [
        "Test the Model's output shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuMME-ieRoWE"
      },
      "outputs": [],
      "source": [
        "x = torch.randn((3,3,128,128))\n",
        "x.shape, x[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_7VClooSQBm"
      },
      "outputs": [],
      "source": [
        "samp = UNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_HjPRXuTPW6"
      },
      "outputs": [],
      "source": [
        "samp(x).shape, samp(x)[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywksBO3iZAIH"
      },
      "source": [
        "### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCU8aqWZZCgy"
      },
      "outputs": [],
      "source": [
        "def train_model(train_loader,model,opt):\n",
        "  epoch_loss = 0.0\n",
        "  model.train()\n",
        "  loop = tqdm(train_loader)\n",
        "  for i, (X,y) in enumerate(loop):\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    # X, y = X.float(), y.float() is not needed as they are already of float dtype\n",
        "    y_pred = model(X)\n",
        "    loss = criterion(y_pred,y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    loop.set_postfix(loss = loss.item())\n",
        "  return epoch_loss/len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIP5umsn9YYs",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Testing the train model class\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = UNet()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "epoch_loss = 0.0\n",
        "model.train()\n",
        "loop = tqdm(train_loader)\n",
        "for i, (X,y) in enumerate(loop):\n",
        "  y_pred = model(X)\n",
        "  print (f'Prediction is {y_pred} \\n max is {torch.max(y_pred)}, min is {torch.min(y_pred)}')\n",
        "  print (f'Actual is {y} \\n max is {torch.max(y)}, min is {torch.min(y)}')\n",
        "\n",
        "  loss = criterion(y_pred,y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  epoch_loss += loss.item()\n",
        "  loop.set_postfix(loss = loss.item())\n",
        "  if i==2:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYvtt6Dy_Y5p"
      },
      "source": [
        "***Loss was becoming negative for the first run***\n",
        "\n",
        "**Reason**: BCEWithLogitsLoss(predictions, actuals) needs predictions and actuals to belong in the range (-inf, inf) and actuals in (0,1).  In my case, input y was not normalized, hence dividing value by 255.0 becomes important"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OyQxtMJa4Ul"
      },
      "outputs": [],
      "source": [
        "def eval_model(test_loader,model):\n",
        "  epoch_loss = 0.0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    loop = tqdm(test_loader)\n",
        "    for i,(X,y) in enumerate(loop):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      y_pred = model(X)\n",
        "      loss = criterion(y_pred,y)\n",
        "      epoch_loss += loss.item()\n",
        "      loop.set_postfix(loss = loss.item())\n",
        "  return epoch_loss/len(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GL49x60aiG3"
      },
      "source": [
        "### ToDo: About Dice Loss:\n",
        "\n",
        "Dice coeff. = 2*Overlap between Pred versus actual/Area of Pred + Actual\n",
        "Dice Loss = 1 - Dice Coeff.\n",
        "\n",
        "Here, for every iteration, 2*overlap further decreases the loss compared to IoU, making it a better metric for obj. segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nzQj5uRMelo"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print (device)\n",
        "model = UNet()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2aBeRCJdiqo"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJCgW24fMWxv"
      },
      "outputs": [],
      "source": [
        "# objective of loss function is not to minimize loss, but to bring it to 0\n",
        "train, val, best_val_loss = [],[], float('inf')\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = train_model(train_loader,model,optimizer)\n",
        "  train.append(train_loss)\n",
        "  val_loss = eval_model(test_loader,model)\n",
        "  val.append(val_loss)\n",
        "\n",
        "  if best_val_loss > val_loss:\n",
        "    print (f'Best Validation loss is reduced from {best_val_loss} to {val_loss}')\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(),'/content/drive/MyDrive/Colab Notebooks/WaterBody_ImageSegmentation/BestModel.pt')\n",
        "\n",
        "  print (f'For epoch no.{epoch}')\n",
        "  print (f'Train loss:{train_loss}, Validation loss:{val_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwGQjBg7pBO4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bCv7b0-2BbmN",
        "X7x1kvna7VFc",
        "uzrG9aba96Tu",
        "QYpPdmhuwEx1",
        "5VwcgSqtxLZS",
        "zs44cedW-j2T",
        "dNcYFW5FVO68"
      ],
      "gpuType": "T4",
      "mount_file_id": "10B8MHt8pV-herZs9E0DRbuxydKQWP6u1",
      "authorship_tag": "ABX9TyPG8B6lQACubmwnNQACcJ0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}